{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b39c44-082a-47d1-8c24-26a032b689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm,trange\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import ujson as json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "from model_balanceloss import DocREModel\n",
    "from utils_sample import set_seed, collate_fn\n",
    "from evaluation import to_official, official_evaluate\n",
    "from prepro import ReadDataset\n",
    "from train_balanceloss_reset import evaluate_o\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import collections\n",
    "\n",
    "\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "rel2id_path = 'dataset/meta/rel2id.json'\n",
    "docred_rel2id = json.load(open(rel2id_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c73e1c-67c2-43ed-944c-59e6763413d6",
   "metadata": {},
   "source": [
    "# Load tokenizer and new keyword dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beabee06-2497-458e-99c1-839afebaf981",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 644)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_keys_new = json.load(open('dataset/docred/dev_keys_new.json'))\n",
    "kdict = pickle.load(open('dataset/docred/keywords_dict.pkl','rb'))\n",
    "model_type = 'roberta-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "len(dev_keys_new), len(kdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255ba7f-c005-4809-a538-868e580e9852",
   "metadata": {},
   "source": [
    "## model prediction for new dev keyword dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c1b0e8-28da-447e-9690-f482a644546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(args, model, features):\n",
    "\n",
    "    dataloader = DataLoader(features, batch_size=args.test_batch_size, shuffle=False, collate_fn=collate_fn, drop_last=False)\n",
    "    preds = []\n",
    "#     print('test feature size: ', len(features))\n",
    "    for batch in tqdm(dataloader):\n",
    "        model.eval()\n",
    "\n",
    "        inputs = {'input_ids': batch[0].to(args.device),\n",
    "                  'attention_mask': batch[1].to(args.device),\n",
    "                  'entity_pos': batch[3],\n",
    "                  'hts': batch[4],\n",
    "                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(**inputs)\n",
    "            pred = pred.cpu().numpy()\n",
    "            pred[np.isnan(pred)] = 0\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0).astype(np.float32)\n",
    "    preds = to_official(preds, features)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def arg_pre():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    \n",
    "    parser.add_argument(\"--data_dir\", default=\"./dataset/docred\", type=str)\n",
    "    parser.add_argument(\"--transformer_type\", default=\"roberta\", type=str)\n",
    "    parser.add_argument(\"--model_name_or_path\", default=\"roberta-large\", type=str)\n",
    "\n",
    "    parser.add_argument(\"--load_path\", default=\"checkpoint/docred/roberta.pt\", type=str)\n",
    "\n",
    "    parser.add_argument(\"--config_name\", default=\"\", type=str,\n",
    "                        help=\"Pretrained config name or path if not the same as model_name\")\n",
    "    parser.add_argument(\"--tokenizer_name\", default=\"\", type=str,\n",
    "                        help=\"Pretrained tokenizer name or path if not the same as model_name\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=1024, type=int,\n",
    "                        help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                             \"than this will be truncated, sequences shorter will be padded.\")\n",
    "\n",
    "    parser.add_argument(\"--test_batch_size\", default=8, type=int,\n",
    "                        help=\"Batch size for testing.\")\n",
    "   \n",
    "    parser.add_argument(\"--seed\", type=int, default=66,\n",
    "                        help=\"random seed for initialization\")\n",
    "    parser.add_argument(\"--num_class\", type=int, default=97,\n",
    "                        help=\"Number of relation types in dataset.\")\n",
    "    parser.add_argument(\"--num_labels\", default=4, type=int,\n",
    "                        help=\"Max number of labels in prediction.\")\n",
    "\n",
    "    parser.add_argument(\"--unet_in_dim\", type=int, default=3,\n",
    "                        help=\"unet_in_dim.\")\n",
    "    parser.add_argument(\"--unet_out_dim\", type=int, default=256,\n",
    "                        help=\"unet_out_dim.\")\n",
    "    parser.add_argument(\"--down_dim\", type=int, default=256,\n",
    "                        help=\"down_dim.\")\n",
    "    parser.add_argument(\"--channel_type\", type=str, default='context-based',\n",
    "                        help=\"unet_out_dim.\")\n",
    "    parser.add_argument(\"--log_dir\", type=str, default='',\n",
    "                        help=\"log.\")\n",
    "    parser.add_argument(\"--max_height\", type=int, default=42,\n",
    "                        help=\"log.\")\n",
    "    parser.add_argument(\"--train_from_saved_model\", type=str, default='',\n",
    "                        help=\"train from a saved model.\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='docred',\n",
    "                        help=\"dataset type\")\n",
    "\n",
    "    args = parser.parse_args(args=[]) # for jupyter execution\n",
    "#     wandb.init(project=\"DocRED\",mode='disabled')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60b57a64-a06a-4016-bae6-4dfbf63013d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from  checkpoint/docred/roberta_reemb.pt\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "args = arg_pre()\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "args.model_name_or_path = model_type\n",
    "if model_type.startswith('r'):\n",
    "    args.transformer_type = 'roberta'\n",
    "    args.load_path = args.load_path.replace('model_bert','model_roberta')\n",
    "else:\n",
    "    args.transformer_type = 'bert'\n",
    "args.load_path = 'checkpoint/docred/roberta_reemb.pt'\n",
    "PRETRAINED_DIR = '/cpfs/user/cht/cbsp/'\n",
    "config = AutoConfig.from_pretrained(\n",
    "    args.config_name if args.config_name else PRETRAINED_DIR+ args.model_name_or_path,\n",
    "    num_labels=args.num_class,\n",
    ")\n",
    "config.output_attentions = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.tokenizer_name if args.tokenizer_name else PRETRAINED_DIR+args.model_name_or_path,\n",
    ")\n",
    "\n",
    "Dataset = ReadDataset(args.dataset, tokenizer, args.max_seq_length)\n",
    "model = AutoModel.from_pretrained(\n",
    "    PRETRAINED_DIR+args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "config.cls_token_id = tokenizer.cls_token_id\n",
    "config.sep_token_id = tokenizer.sep_token_id\n",
    "config.transformer_type = args.transformer_type\n",
    "\n",
    "set_seed(args)\n",
    "model = DocREModel(config, args,  model, num_labels=args.num_labels)\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(args.load_path)['checkpoint'])\n",
    "print('load model from ', args.load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0eb92-6e5e-4466-8c01-5483b2e02cfa",
   "metadata": {},
   "source": [
    "# Load keyword attack ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be4917e5-bb79-48ba-a733-51da3b7bf049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 7342/7342 [01:40<00:00, 73.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 7342.\n",
      "# of positive examples 7342.\n",
      "# of negative examples 0.\n",
      "# 206 examples len>512 and max len is 732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 7342/7342 [01:37<00:00, 75.24it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 7342.\n",
      "# of positive examples 7342.\n",
      "# of negative examples 0.\n",
      "# 206 examples len>512 and max len is 732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 2002/2002 [00:26<00:00, 75.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 2002.\n",
      "# of positive examples 2002.\n",
      "# of negative examples 0.\n",
      "# 45 examples len>512 and max len is 715.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 2002/2002 [00:27<00:00, 72.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 2002.\n",
      "# of positive examples 2002.\n",
      "# of negative examples 0.\n",
      "# 45 examples len>512 and max len is 714.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 5231/5231 [01:11<00:00, 72.94it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 5231.\n",
      "# of positive examples 5231.\n",
      "# of negative examples 0.\n",
      "# 146 examples len>512 and max len is 732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 5231/5231 [01:13<00:00, 71.36it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 5231.\n",
      "# of positive examples 5231.\n",
      "# of negative examples 0.\n",
      "# 146 examples len>512 and max len is 732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7342, 2002, 5231)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import prepro\n",
    "importlib.reload(prepro)\n",
    "from prepro import read_docred_att\n",
    "attack_dir = 'attack_ds/'\n",
    "ori_path = attack_dir + model_type + '@ori_keyword_dev.json'\n",
    "mask_path = attack_dir + model_type + '@mask_keyword_dev.json'\n",
    "anto_path = attack_dir + model_type + '@anto_keyword_dev.json'\n",
    "ori_anto_path = attack_dir + model_type + '@ori_anto_keyword_dev.json'\n",
    "syno_path = attack_dir + model_type + '@syno_keyword_dev.json'\n",
    "ori_syno_path = attack_dir + model_type + '@ori_syno_keyword_dev.json'\n",
    "\n",
    "ori_features = read_docred_att(ori_path, tokenizer,ast=False)\n",
    "mask_features = read_docred_att(mask_path, tokenizer,ast=False)\n",
    "anto_features = read_docred_att(anto_path, tokenizer,ast=False)\n",
    "ori_anto_features = read_docred_att(ori_anto_path, tokenizer,ast=False)\n",
    "syno_features = read_docred_att(syno_path, tokenizer,ast=False)\n",
    "ori_syno_features = read_docred_att(ori_syno_path, tokenizer,ast=False)\n",
    "\n",
    "len(ori_features), len(anto_features), len(syno_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99b86e03-2dcb-4d31-9a83-ebb9f9404e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_keyword(features):\n",
    "    pred = report(args, model, features)\n",
    "    print(len(pred))\n",
    "    spreds = sorted(pred, key=lambda x:x['title'])\n",
    "    spreds = [{'title':s['title'], 'r': docred_rel2id[s['r']]} for s in spreds]\n",
    "    key_res = defaultdict(list)\n",
    "    for s in spreds:\n",
    "        key_res[s['title']].append(s['r'])\n",
    "    all_rnum = len(features)\n",
    "    pos_rnum = len(key_res)\n",
    "    no_rnum = all_rnum - pos_rnum\n",
    "    true_rnum = 0\n",
    "    for title, rl in key_res.items():\n",
    "        truth = int(title.split('_')[-1])\n",
    "        if truth in rl:\n",
    "            true_rnum += 1\n",
    "\n",
    "    wrong_rnum = pos_rnum - true_rnum\n",
    "    return key_res, all_rnum, pos_rnum, no_rnum, true_rnum, wrong_rnum\n",
    "\n",
    "def attack_ratio(ori_key_res, key_res, features):\n",
    "    # ori no rel, now rel\n",
    "    all_titles = set([f['title'] for f in features])\n",
    "    ori_titles = set(ori_key_res.keys())\n",
    "    titles = set(key_res.keys())\n",
    "    no_titles = all_titles - ori_titles\n",
    "    nor_r_ratio = len(no_titles & titles) / len(no_titles)\n",
    "    # ori rel, now no rel\n",
    "    r_nor_ratio = len((all_titles - titles) & ori_titles) / len(ori_titles)\n",
    "    \n",
    "    no_num,true_num,false_num = 0,0,0\n",
    "    for key in ori_key_res.keys():\n",
    "        if key not in key_res:\n",
    "            no_num += 1\n",
    "        elif len(set(key_res[key]) & set(ori_key_res[key])) > 0:\n",
    "            true_num += 1\n",
    "        else:\n",
    "            false_num += 1\n",
    "    # ori one rel, now another rel\n",
    "    rel_arel_ratio = false_num / len(ori_titles)\n",
    "    # ori one rel, now rel covers\n",
    "    rel_srel_ratio = true_num / len(ori_titles)\n",
    "    \n",
    "    return r_nor_ratio, nor_r_ratio, rel_arel_ratio, rel_srel_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ee35d-7b02-42ba-baa1-847656d91975",
   "metadata": {},
   "source": [
    "# Entity Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11903ac0-2353-475e-bfea-68716ae0e05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 884/884 [00:25<00:00, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 884.\n",
      "# of positive examples 10295.\n",
      "# of negative examples 335057.\n",
      "# 50 examples len>512 and max len is 804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 884/884 [00:22<00:00, 40.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 884.\n",
      "# of positive examples 10295.\n",
      "# of negative examples 335057.\n",
      "# 50 examples len>512 and max len is 804.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 884/884 [00:22<00:00, 39.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 884.\n",
      "# of positive examples 10295.\n",
      "# of negative examples 335057.\n",
      "# 51 examples len>512 and max len is 841.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 884/884 [00:18<00:00, 48.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 884.\n",
      "# of positive examples 10295.\n",
      "# of negative examples 335057.\n",
      "# 44 examples len>512 and max len is 790.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(884, 884, 884)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_balanceloss_reset import evaluate_o\n",
    "attack_dir = 'attack_ds/'\n",
    "# json.dump(ori_features, open(attack_dir + model_type + '@ori_keyword_dev.pkl', 'wb'))\n",
    "ori_file_path = attack_dir + 'dev_wo_overlap.json'\n",
    "en_mask_path = attack_dir + model_type + '@en_mask_dev.json'\n",
    "en_shuf_path = attack_dir + model_type + '@en_shuf_dev.json'\n",
    "en_repl_path = attack_dir + model_type + '@en_repl_dev.json'\n",
    "\n",
    "ori_features = read_docred_att(ori_file_path, tokenizer)\n",
    "en_mask_features = read_docred_att(en_mask_path, tokenizer)\n",
    "en_shuf_features = read_docred_att(en_shuf_path, tokenizer)\n",
    "en_repl_features = read_docred_att(en_repl_path, tokenizer)\n",
    "\n",
    "len(en_mask_features), len(en_shuf_features), len(en_repl_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91f8ac62-fe51-430b-b0b0-e4f2ff049076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(channel_type='context-based', config_name='', data_dir='./dataset/docred', dataset='docred', dev_file='dev_wo_overlap.json', device=device(type='cuda', index=0), down_dim=256, load_path='checkpoint/docred/roberta_reemb.pt', log_dir='', max_height=42, max_seq_length=1024, model_name_or_path='roberta-large', n_gpu=2, num_class=97, num_labels=4, seed=66, test_batch_size=8, tokenizer_name='', train_from_saved_model='', transformer_type='roberta', unet_in_dim=3, unet_out_dim=256)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.dev_file = 'dev_wo_overlap.json'\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db8e1728-31e1-44c8-b868-2adba5bd15a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 63.29041487839772, 'dev_F1_ign': 61.40522004808886, 'dev_re_p': 66.63319610402651, 'dev_re_r': 60.26700572155118, 'dev_average_loss': 0.37415633760057054}\n",
      "{'dev_F1': 8.622610782549069, 'dev_F1_ign': 8.610971727686167, 'dev_re_p': 76.67682926829268, 'dev_re_r': 4.568159113613659, 'dev_average_loss': 1.066540204726898}\n",
      "{'dev_F1': 8.077393787516755, 'dev_F1_ign': 7.670949823935591, 'dev_re_p': 11.27195836044242, 'dev_re_r': 6.293706293706294, 'dev_average_loss': 1.2295448796169177}\n",
      "{'dev_F1': 18.54648059177915, 'dev_F1_ign': 18.449231939435034, 'dev_re_p': 57.84966698382493, 'dev_re_r': 11.043501952592862, 'dev_average_loss': 0.9096437628204758}\n",
      "\n",
      "& original & 63.29041487839772 & 61.40522004808886 \\\\ \n",
      "& entity mask & 8.622610782549069 & 8.610971727686167 \\\\ \n",
      "& entity move & 8.077393787516755 & 7.670949823935591 \\\\ \n",
      "& entity replace & 18.54648059177915 & 18.449231939435034 \\\\ \n"
     ]
    }
   ],
   "source": [
    "# Docu ori roberta-large\n",
    "all_feas = [ori_features, en_mask_features, en_shuf_features, en_repl_features] # ori_features, \n",
    "f1_outs = []\n",
    "for fea in all_feas:\n",
    "    _, f1_out = evaluate_o(args, model, fea, tag=\"dev\")\n",
    "    print(f1_out)\n",
    "    f1_outs.append(f1_out)\n",
    "en_attack_strs = ['original','entity mask', 'entity move', 'entity replace']\n",
    "print()\n",
    "for i in range(0, len(f1_outs)):\n",
    "    print(f'& {en_attack_strs[i]} & {f1_outs[i][\"dev_F1\"]} & {f1_outs[i][\"dev_F1_ign\"]} \\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d9594c-d2b4-496f-84ee-2aab094e698e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlop",
   "language": "python",
   "name": "atlop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
