{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b39c44-082a-47d1-8c24-26a032b689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import trange, tqdm\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.misc import derivative\n",
    "import pickle\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691f4b0-8cb2-486b-933a-4fbff7f61266",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 1024\n",
    "\n",
    "rel2id_path = 'dataset/meta/rel2id.json'\n",
    "docred_rel2id = json.load(open(rel2id_path))\n",
    "id2rel = {v:k for k,v in docred_rel2id.items()}\n",
    "dev_keys_new = json.load(open(\"dataset/docred/dev_keys_new.json\"))\n",
    "kdict = pickle.load(open(\"dataset/docred/keywords_dict.pkl\",\"rb\"))\n",
    "\n",
    "model_type = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36eb181a-034a-43c3-96d2-bf79513cdc11",
   "metadata": {},
   "source": [
    "# MAP metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603cc9ee-52df-4648-998e-4875b5be433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read IG val list\n",
    "ig_atl_all_list = pickle.load(open('../ATLOP/infers/dev_keys_new_ig_all@roberta.pkl','rb'))\n",
    "ig_docu_all_list = pickle.load(open('../DocuNet/infers/docu_dev_keys_new_ig_all.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4214cee-d4e3-472a-b4e7-b72a8f9e8ac0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_TOPK = 100\n",
    "MIN_VAL = -1000000\n",
    "def sharpen(arr, T):\n",
    "    T = 1 / T\n",
    "    sum_arr = np.sum(np.power(arr, T), axis=0)\n",
    "    return arr / sum_arr\n",
    "def cal_mAP(val_all_list, topk, limit=True, offset=0):\n",
    "    data = dev_keys_new\n",
    "    stop_i = len(val_all_list) # for partial calculation\n",
    "    no_num = 0\n",
    "    mAPs = []\n",
    "    old_mAPs = []\n",
    "    sharps, max_topks = [],[]\n",
    "    if limit:\n",
    "        data = data[:10]\n",
    "        data= tqdm(data, desc=\"Keyword Search\")\n",
    "    for si, sample in enumerate(data):\n",
    "        if si >= stop_i: break\n",
    "        sents = []\n",
    "        sent_map = []\n",
    "        entities = sample['vertexSet']\n",
    "        entity_start, entity_end = [], []\n",
    "        all_mentions = []\n",
    "\n",
    "        for e_list in entities:\n",
    "            all_mentions += e_list\n",
    "        for entity in entities:\n",
    "            for mention in entity:\n",
    "                sent_id = mention[\"sent_id\"]\n",
    "                pos = mention[\"pos\"]\n",
    "                entity_start.append((sent_id, pos[0],))\n",
    "                entity_end.append((sent_id, pos[1] - 1,))\n",
    "        for i_s, sent in enumerate(sample['sents']):\n",
    "            new_map = {}\n",
    "            for i_t, token in enumerate(sent):\n",
    "                tokens_wordpiece = tokenizer.tokenize(token)\n",
    "                if (i_s, i_t) in entity_start:\n",
    "                    tokens_wordpiece = [\"*\"] + tokens_wordpiece  # add * around entity\n",
    "                if (i_s, i_t) in entity_end:\n",
    "                    tokens_wordpiece = tokens_wordpiece + [\"*\"]\n",
    "                new_map[i_t] = len(sents)\n",
    "                sents.extend(tokens_wordpiece)\n",
    "            new_map[i_t + 1] = len(sents)\n",
    "            sent_map.append(new_map)\n",
    "        train_triple = {}\n",
    "        if \"labels\" in sample:\n",
    "            for label in sample['labels']:\n",
    "                evidence = label['evidence'] if 'evidence' in label else []\n",
    "                r = int(docred_rel2id[label['r']])\n",
    "                if (label['h'], label['t']) not in train_triple:\n",
    "                    train_triple[(label['h'], label['t'])] = [\n",
    "                        {'relation': r, 'evidence': evidence}]\n",
    "                else:\n",
    "                    train_triple[(label['h'], label['t'])].append(\n",
    "                        {'relation': r, 'evidence': evidence})\n",
    "        entity_pos = []\n",
    "        for e in entities:\n",
    "            entity_pos.append([])\n",
    "            for m in e:\n",
    "                start = sent_map[m[\"sent_id\"]][m[\"pos\"][0]]\n",
    "                end = sent_map[m[\"sent_id\"]][m[\"pos\"][1]]\n",
    "                entity_pos[-1].append((start, end,))\n",
    "        relations, hts = [], []\n",
    "        for h, t in train_triple.keys():\n",
    "            relation = [0] + [0] * len(docred_rel2id)\n",
    "            for mention in train_triple[h, t]:\n",
    "                relation[mention[\"relation\"]] = 1\n",
    "                evidence = mention[\"evidence\"]\n",
    "            relations.append(relation)\n",
    "            hts.append([h, t])\n",
    "\n",
    "        key_dict = defaultdict(list)\n",
    "        for h,t,r,sent_id,st,ed,name in kdict[sample['title']]:\n",
    "            key_dict[(h,t,r)].append((sent_id, st, ed, name))\n",
    "        for h,t,r in key_dict.keys():\n",
    "            ht_i = hts.index([h,t])\n",
    "            ig_vals, topk_indices = val_all_list[si][0][ht_i], val_all_list[si][1][ht_i].tolist()\n",
    "            \n",
    "            ro = r + offset\n",
    "            if ro not in topk_indices:  # skip wrong grad prediction\n",
    "                continue\n",
    "            \n",
    "            #ind_topk = np.argpartition(ig_vals[topk_indices.index(r)], -topk, axis=-1)[-topk:].tolist()\n",
    "            ind_topk = np.argsort(ig_vals[topk_indices.index(ro)], axis=-1)[-topk:].tolist()\n",
    "            ind_topk.reverse()\n",
    "            max_ind_topk = np.argpartition(ig_vals[topk_indices.index(ro)], -MAX_TOPK, axis=-1)[-MAX_TOPK:].tolist()\n",
    "            max_ind_topk.reverse()\n",
    "            # new MAP\n",
    "            topk_np = ig_vals[topk_indices.index(ro), max_ind_topk]\n",
    "            topk_sharp = np.flip(np.sort(sharpen(topk_np, 0.5)))\n",
    "            sharps.append(topk_sharp)\n",
    "            \n",
    "            fx=np.linspace(0,MAX_TOPK,MAX_TOPK)\n",
    "            f=interp1d(fx, topk_sharp,kind=\"cubic\")\n",
    "            dx = 1/10\n",
    "            one=np.empty((MAX_TOPK))\n",
    "            two=np.empty((MAX_TOPK))\n",
    "            for fi in range(MAX_TOPK):\n",
    "                one[fi]=derivative(f,0 + dx+dx*fi, dx) # second param is derivative position\n",
    "                two[fi]=derivative(f,0 + dx+dx*fi, dx, n=2)\n",
    "            tt=np.linspace(0, MAX_TOPK-dx,MAX_TOPK)\n",
    "            # find point x that one(x) ~= 0 and two(x) ~> 0\n",
    "            mtopk = 0\n",
    "            find_one = False\n",
    "            for i in range(len(one) - 1):\n",
    "                if one[i] * one[i+1] < 0:\n",
    "                    find_one = True\n",
    "                    mtopk = i\n",
    "                    break\n",
    "            if not find_one: \n",
    "                # to find 0 nearest value\n",
    "                mtopk = np.argmin(np.abs(one))\n",
    "            #print(f'not found one==0, sub: {mtopk}')\n",
    "                no_num += 1\n",
    "            for i in range(mtopk,len(two) - 1):\n",
    "                if two[i] <0 and two[i] * two[i+1] <0:\n",
    "                    max_topks.append(i)\n",
    "                    mtopk = i\n",
    "                    break\n",
    "            \n",
    "            if len(ind_topk) != topk:\n",
    "                raise ValueError\n",
    "            key_inds = []\n",
    "            for sent_id, st, ed, _ in key_dict[(h,t,r)]:\n",
    "                st, ed = sent_map[sent_id][st], sent_map[sent_id][ed]\n",
    "                key_inds.extend(range(st, ed))\n",
    "            key_inds = set(key_inds)\n",
    "            ap,old_ap = [],[]\n",
    "            for pos in range(len(ind_topk)):\n",
    "                if ind_topk[pos] in key_inds:\n",
    "                    pp = (pos+1) if pos <= mtopk else (mtopk+1)\n",
    "                    ap.append(1 / pp)\n",
    "                    old_ap.append(1/(pos+1))\n",
    "            mAPs.append(sum(ap) / topk)\n",
    "            old_mAPs.append(sum(old_ap)/topk)\n",
    "    return sum(mAPs)/len(mAPs) if len(mAPs) >0 else 0, sum(old_mAPs)/len(old_mAPs), len(mAPs), max_topks, no_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ec5c61-981e-42cc-88fa-7572479f819b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ig_atl_nmap_stat, ig_docu_nmap_stat = [],[]\n",
    "for topk in trange(1, MAX_TOPK+1):\n",
    "\n",
    "    # mAP\n",
    "    mAPs,old_mAPs = cal_mAP(ig_atl_all_list, topk, limit=False)[:2]\n",
    "    ig_atl_nmap_stat.append(mAPs)\n",
    "\n",
    "    mAPs,old_mAPs = cal_mAP(ig_docu_all_list, topk, limit=False)[:2]\n",
    "    ig_docu_nmap_stat.append(mAPs)\n",
    "\n",
    "\n",
    "pickle.dump(ig_atl_nmap_stat, open('keyword_pkl/ig_atl_nmap_stat.pkl','wb'))\n",
    "pickle.dump(ig_docu_nmap_stat, open('keyword_pkl/ig_docu_nmap_stat.pkl','wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlop",
   "language": "python",
   "name": "atlop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
