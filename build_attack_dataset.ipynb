{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b39c44-082a-47d1-8c24-26a032b689d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython import embed\n",
    "from collections import defaultdict, Counter\n",
    "import copy\n",
    "import pickle\n",
    "import spacy as sp\n",
    "import networkx as nx\n",
    "import os\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, BartModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import importlib\n",
    "import prepro\n",
    "importlib.reload(prepro)\n",
    "from prepro import read_docred\n",
    "\n",
    "BERT_DIR = '/cpfs/user/cht/cbsp/'\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "rel2id_path = 'dataset/meta/rel2id.json'\n",
    "rel2id = json.load(open(rel2id_path))\n",
    "id2rel = {v:k for k,v in rel2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c73e1c-67c2-43ed-944c-59e6763413d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load tokenizer and new keyword dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beabee06-2497-458e-99c1-839afebaf981",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(699, 644)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_keys_new = json.load(open('dataset/docred/dev_keys_new.json'))\n",
    "kdict = pickle.load(open('dataset/docred/keywords_dict.pkl','rb'))\n",
    "model_type = 'roberta-large'  # 'bert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_DIR + model_type)\n",
    "len(dev_keys_new), len(kdict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fd20d4-b550-48db-9bde-29d0662af4fd",
   "metadata": {},
   "source": [
    "# Build keyword attack dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a8d7d960-3c9a-49ff-a066-06fca386537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P6': 1, 'P17': 2, 'P19': 3, 'P20': 4, 'P22': 5, 'P25': 6, 'P26': 7, 'P27': 8, 'P30': 9, 'P31': 10, 'P35': 11, 'P36': 12, 'P37': 13, 'P39': 14, 'P40': 15, 'P50': 16, 'P54': 17, 'P57': 18, 'P58': 19, 'P69': 20, 'P86': 21, 'P102': 22, 'P108': 23, 'P112': 24, 'P118': 25, 'P123': 26, 'P127': 27, 'P131': 28, 'P136': 29, 'P137': 30, 'P140': 31, 'P150': 32, 'P155': 33, 'P156': 34, 'P159': 35, 'P161': 36, 'P162': 37, 'P166': 38, 'P170': 39, 'P171': 40, 'P172': 41, 'P175': 42, 'P176': 43, 'P178': 44, 'P179': 45, 'P190': 46, 'P194': 47, 'P205': 48, 'P206': 49, 'P241': 50, 'P264': 51, 'P272': 52, 'P276': 53, 'P279': 54, 'P355': 55, 'P361': 56, 'P364': 57, 'P400': 58, 'P403': 59, 'P449': 60, 'P463': 61, 'P488': 62, 'P495': 63, 'P527': 64, 'P551': 65, 'P569': 66, 'P570': 67, 'P571': 68, 'P576': 69, 'P577': 70, 'P580': 71, 'P582': 72, 'P585': 73, 'P607': 74, 'P674': 75, 'P676': 76, 'P706': 77, 'P710': 78, 'P737': 79, 'P740': 80, 'P749': 81, 'P800': 82, 'P807': 83, 'P840': 84, 'P937': 85, 'P1001': 86, 'P1056': 87, 'P1198': 88, 'P1336': 89, 'P1344': 90, 'P1365': 91, 'P1366': 92, 'P1376': 93, 'P1412': 94, 'P1441': 95, 'P3373': 96}\n"
     ]
    }
   ],
   "source": [
    "print(docred_rel2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e7309373-03e0-4817-b81a-a49ecbfdffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 699/699 [00:09<00:00, 75.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 0.\n",
      "# of original num 7342.\n",
      "# of mask num 7342.\n",
      "# of antonym num 2002.\n",
      "# of synonym num 5231. ref count 5231\n"
     ]
    }
   ],
   "source": [
    "file_in = 'dataset/docred/dev_keys_new.json'\n",
    "i_line, pos_samples, neg_samples = 0,0,0\n",
    "ori_rel_pairs = []\n",
    "# mask_features is model_type specific\n",
    "ori_features, mask_features, ori_anto_features, anto_features, ori_syno_features, syno_features = [],[],[],[],[],[]\n",
    "mask_num, anto_num, syno_num, ori_num = 0,0,0,0\n",
    "\n",
    "limit = False\n",
    "with open(file_in, \"r\") as fh:\n",
    "    data = json.load(fh)\n",
    "if limit:\n",
    "    data = data[:10]\n",
    "\n",
    "for sample in tqdm(data, desc=\"Example\"):\n",
    "    sents = sample['sents']\n",
    "    entities = sample['vertexSet']\n",
    "    train_triple = {}\n",
    "    htr2label = {}\n",
    "    if \"labels\" in sample:\n",
    "        for label in sample['labels']:\n",
    "            evidence = label['evidence'] if 'evidence' in label else []\n",
    "            r = int(docred_rel2id[label['r']])\n",
    "            if (label['h'], label['t']) not in train_triple:\n",
    "                train_triple[(label['h'], label['t'])] = [\n",
    "                    {'relation': r, 'evidence': evidence}]\n",
    "            else:\n",
    "                train_triple[(label['h'], label['t'])].append(\n",
    "                    {'relation': r, 'evidence': evidence})\n",
    "            h,t = label['h'], label['t']\n",
    "            label['h'], label['t'] = 0, 1\n",
    "            htr2label[(h,t, r)] = label\n",
    "\n",
    "    relations, hts = [], []\n",
    "    for h, t in train_triple.keys():\n",
    "        relation = [0] + [0] * len(docred_rel2id)\n",
    "        for mention in train_triple[h, t]:\n",
    "            relation[mention[\"relation\"]] = 1\n",
    "            evidence = mention[\"evidence\"]\n",
    "        relations.append(relation)\n",
    "        hts.append([h, t])\n",
    "        pos_samples += 1\n",
    "\n",
    "    key_dict = defaultdict(list)\n",
    "    for h,t,r,sent_id,st,ed,name in kdict[sample['title']]:\n",
    "        key_dict[(h,t,r)].append((sent_id, st, ed, name))\n",
    "    for h,t,r in key_dict.keys():\n",
    "        if [h,t] not in hts: continue\n",
    "        mask_sents, anto_sents, syno_sents = deepcopy(sents),deepcopy(sents),deepcopy(sents)\n",
    "        anto_index_now, syno_index_now = 0, 0\n",
    "        has_anto, has_syno = False, False\n",
    "\n",
    "        key_dict_sort = sorted(key_dict[(h,t,r)], key=lambda x: x[0])\n",
    "        for sent_id, st, ed, name in key_dict_sort:\n",
    "            # do masking strategy\n",
    "            for po in range(st,ed):\n",
    "                tok_len = len(tokenizer.tokenize(sents[sent_id][po]))\n",
    "                mask_sents[sent_id][po] = tokenizer.mask_token * tok_len\n",
    "\n",
    "            # do wordnet synonyms and antonym replace strategy\n",
    "            antonyms, synonyms = [], []\n",
    "            if name == None: continue  # wrong case\n",
    "            name = name.split() \n",
    "            # only consider first word\n",
    "            for syn in wn.synsets(name[0]):\n",
    "                for l in syn.lemmas():\n",
    "                    synonyms.append(l.name())\n",
    "                    if l.antonyms():\n",
    "                        antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "            # antonyms\n",
    "            if len(antonyms) > 0:\n",
    "                antonym = antonyms[0]  # first antonym\n",
    "                if antonym != name:\n",
    "                    assert len(antonym.split()) == 1\n",
    "                    anto_sents[sent_id][st] = antonym\n",
    "                    has_anto = True\n",
    "\n",
    "            # synonyms\n",
    "            if len(synonyms) > 0:\n",
    "                synonym = synonyms[0]  # first antonym\n",
    "                if synonym != name:\n",
    "                    assert len(synonym.split()) == 1\n",
    "                    syno_sents[sent_id][st] = synonym\n",
    "                    has_syno = True\n",
    "\n",
    "\n",
    "        ori_rel_pairs.append((entities[h][0]['name'], entities[t][0]['name'], id2rel[r]))\n",
    "        ht_entities = [entities[h], entities[t]]\n",
    "        ht_labels = [htr2label[(h,t,r)]]\n",
    "        ht_title = f'{sample[\"title\"]}_{h}_{t}_{r}'\n",
    "\n",
    "\n",
    "        # original feature construction\n",
    "        ori_dict = {\n",
    "            'vertexSet': ht_entities,\n",
    "            'labels': ht_labels,\n",
    "            'title': ht_title,\n",
    "            'sents': sents\n",
    "        }\n",
    "        ori_features.append(ori_dict)\n",
    "        ori_num += 1\n",
    "\n",
    "        # mask feature construction\n",
    "        mask_dict = {\n",
    "            'vertexSet': ht_entities,\n",
    "            'labels': ht_labels,\n",
    "            'title': ht_title,\n",
    "            'sents': mask_sents\n",
    "        }\n",
    "        mask_features.append(mask_dict)\n",
    "        mask_num += 1\n",
    "\n",
    "        # replace feature construction\n",
    "        if has_anto:\n",
    "            anto_dict = {\n",
    "                'vertexSet': ht_entities,\n",
    "                'labels': ht_labels,\n",
    "                'title': ht_title,\n",
    "                'sents': anto_sents\n",
    "            }\n",
    "            anto_features.append(anto_dict)\n",
    "            anto_num += 1\n",
    "            ori_anto_features.append(ori_dict)\n",
    "\n",
    "        if has_syno:\n",
    "            syno_dict = {\n",
    "                'vertexSet': ht_entities,\n",
    "                'labels': ht_labels,\n",
    "                'title': ht_title,\n",
    "                'sents': syno_sents\n",
    "            }\n",
    "            syno_features.append(syno_dict)\n",
    "            syno_num += 1\n",
    "            ori_syno_features.append(ori_dict)\n",
    "\n",
    "attack_dir = 'attack_pkl/'\n",
    "\n",
    "json.dump(ori_features, open(attack_dir + model_type + '@ori_keyword_dev.json', 'w'))\n",
    "json.dump(mask_features, open(attack_dir + model_type + '@mask_keyword_dev.json', 'w'))\n",
    "json.dump(anto_features, open(attack_dir + model_type + '@anto_keyword_dev.json', 'w'))\n",
    "json.dump(ori_anto_features, open(attack_dir + model_type + '@ori_anto_keyword_dev.json', 'w'))\n",
    "json.dump(syno_features, open(attack_dir + model_type + '@syno_keyword_dev.json', 'w'))\n",
    "json.dump(ori_syno_features, open(attack_dir + model_type + '@ori_syno_keyword_dev.json', 'w'))\n",
    "\n",
    "print(\"# of documents {}.\".format(i_line))\n",
    "print(\"# of original num {}.\".format(ori_num))\n",
    "print(\"# of mask num {}.\".format(mask_num))\n",
    "print(\"# of antonym num {}.\".format(anto_num))\n",
    "print(\"# of synonym num {}. ref count {}\".format(syno_num, len(ori_syno_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3dccf-9d95-43ad-94aa-742391076e29",
   "metadata": {},
   "source": [
    "# Load keyword attack dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5e074f7c-dd0a-416f-ab95-bd2819f65d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta-large'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef283884-599b-4cb1-ada3-e0477d0dc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dir = 'attack_pkl/'\n",
    "ori_path = attack_dir + model_type + '@ori_keyword_dev.json'\n",
    "mask_path = attack_dir + model_type + '@mask_keyword_dev.json'\n",
    "anto_path = attack_dir + model_type + '@anto_keyword_dev.json'\n",
    "ori_anto_path = attack_dir + model_type + '@ori_anto_keyword_dev.json'\n",
    "syno_path = attack_dir + model_type + '@syno_keyword_dev.json'\n",
    "ori_syno_path = attack_dir + model_type + '@ori_syno_keyword_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d599bf2d-109d-4f9b-a0e4-38bd6ba35f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 7342/7342 [02:53<00:00, 42.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 7342.\n",
      "# of positive examples 7342.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 7342/7342 [02:48<00:00, 43.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 7342.\n",
      "# of positive examples 7342.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 2002/2002 [00:46<00:00, 43.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 2002.\n",
      "# of positive examples 2002.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 2002/2002 [00:47<00:00, 42.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 2002.\n",
      "# of positive examples 2002.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 5231/5231 [02:01<00:00, 42.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 5231.\n",
      "# of positive examples 5231.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 5231/5231 [01:56<00:00, 45.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 5231.\n",
      "# of positive examples 5231.\n",
      "# of negative examples 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7342, 2002, 5231)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_features = read_docred(ori_path, tokenizer,ast=False)\n",
    "mask_features = read_docred(mask_path, tokenizer,ast=False)\n",
    "anto_features = read_docred(anto_path, tokenizer,ast=False)\n",
    "ori_anto_features = read_docred(ori_anto_path, tokenizer,ast=False)\n",
    "syno_features = read_docred(syno_path, tokenizer,ast=False)\n",
    "ori_syno_features = read_docred(ori_syno_path, tokenizer,ast=False)\n",
    "\n",
    "len(ori_features), len(anto_features), len(syno_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0b72b0-bb1e-4de5-a740-59e384e9dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_keyword(features):\n",
    "    pred = report(args, model, features)\n",
    "    print(len(pred))\n",
    "    spreds = sorted(pred, key=lambda x:x['title'])\n",
    "    spreds = [{'title':s['title'], 'r': docred_rel2id[s['r']]} for s in spreds]\n",
    "    key_res = defaultdict(list)\n",
    "    for s in spreds:\n",
    "        key_res[s['title']].append(s['r'])\n",
    "    all_rnum = len(features)\n",
    "    pos_rnum = len(key_res)\n",
    "    no_rnum = all_rnum - pos_rnum\n",
    "    true_rnum = 0\n",
    "    for title, rl in key_res.items():\n",
    "        truth = int(title.split('_')[-1])\n",
    "        if truth in rl:\n",
    "            true_rnum += 1\n",
    "\n",
    "    wrong_rnum = pos_rnum - true_rnum\n",
    "    return key_res, all_rnum, pos_rnum, no_rnum, true_rnum, wrong_rnum\n",
    "\n",
    "def attack_ratio(ori_key_res, key_res, ori_features):\n",
    "    # ori no rel, now rel\n",
    "    all_titles = set([f['title'] for f in ori_features])\n",
    "    ori_titles = set(ori_key_res.keys())\n",
    "    titles = set(key_res.keys())\n",
    "    no_titles = all_titles - ori_titles\n",
    "    nor_r_ratio = len(no_titles & titles) / len(no_titles)\n",
    "    # ori rel, now no rel\n",
    "    r_nor_ratio = len((all_titles - titles) & ori_titles) / len(ori_titles)\n",
    "    \n",
    "    no_num,true_num,false_num = 0,0,0\n",
    "    for key in ori_key_res.keys():\n",
    "        if key not in key_res:\n",
    "            no_num += 1\n",
    "        elif len(set(key_res[key]) & set(ori_key_res[key])) > 0:\n",
    "            true_num += 1\n",
    "        else:\n",
    "            false_num += 1\n",
    "    # ori one rel, now another rel\n",
    "    rel_arel_ratio = false_num / len(ori_titles)\n",
    "    # ori one rel, now rel covers\n",
    "    rel_srel_ratio = true_num / len(ori_titles)\n",
    "    \n",
    "    return r_nor_ratio, nor_r_ratio, rel_arel_ratio, rel_srel_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db5515-a317-4be1-a344-c3ce80c73563",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load model to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1592d121-b5ed-4ac2-b4f9-a4b3b79a0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import get_model\n",
    "importlib.reload(get_model)\n",
    "from utils import report\n",
    "from get_model import get_model\n",
    "\n",
    "model = None\n",
    "del model\n",
    "torch.cuda.empty_cache()  # reload model to cuda\n",
    "args, model = get_model(model_type, reset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6bb10-c3d7-4447-9aab-7f81ba3aa343",
   "metadata": {},
   "source": [
    "# Run keyword attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a7ff292-a079-40af-b88c-dabca9f822af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [01:36<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5020\n",
      "7342 4566 2776 4394 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [01:37<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4311\n",
      "7342 3921 3421 3725 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:28<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1285\n",
      "2002 1211 791 1170 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [00:28<00:00,  8.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198\n",
      "2002 1135 867 1074 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [01:11<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3452\n",
      "5231 3169 2062 3050 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 654/654 [01:11<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355\n",
      "5231 3088 2143 2966 122\n",
      "& Mask & 0.16513360 & 0.03926513 & 0.00503723 & 0.82982917 \\\\\n",
      "& Antonym & 0.08422791 & 0.03286979 & 0.01156069 & 0.90421140 \\\\\n",
      "& Synonym & 0.03849795 & 0.01988361 & 0.00126223 & 0.96023982 \\\\\n"
     ]
    }
   ],
   "source": [
    "from attacks import metric_keyword\n",
    "\n",
    "# Attack ratio newer version (only consider keywords that will changed)\n",
    "# roberta-large \n",
    "fs = [ori_features, mask_features, ori_anto_features, anto_features, ori_syno_features, syno_features]\n",
    "key_res_list = []\n",
    "for f in fs:\n",
    "    key_res, all_rnum, pos_rnum, no_rnum, true_rnum, wrong_rnum = metric_keyword(f)\n",
    "    key_res_list.append(key_res)\n",
    "    print(all_rnum, pos_rnum, no_rnum, true_rnum, wrong_rnum)\n",
    "\n",
    "fea_strs = ['Mask', 'Antonym', 'Synonym']\n",
    "# latex format\n",
    "for i in range(0, len(fs),2):\n",
    "    print(\"& %s & %.8f & %.8f & %.8f & %.8f \\\\\\\\\" % ((fea_strs[i//2],) + attack_ratio(key_res_list[i], key_res_list[i+1], fs[i]))) # output as latex table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ed354-5531-4523-bd32-f9cc3f75fe7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5362ecf5-3c1a-40c4-a530-c29519261e87",
   "metadata": {},
   "source": [
    "# Entity Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea019f4f-75a8-45ea-b410-8275e9d8bd1b",
   "metadata": {},
   "source": [
    "## Entity Attack Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821b0b1-24bd-404b-958b-7dee9a728d99",
   "metadata": {},
   "source": [
    "1. entity mask  \n",
    "2. entity shuffle \n",
    "3. entity replacement with entity that does not exist in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea75fc3-95c5-47e0-b7d9-98ad863a064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('repl_ens.pkl', 'rb') as e:\n",
    "    repl_ens = pickle.load(e)\n",
    "docred_rel2id = json.load(open('dataset/meta/rel2id.json'))\n",
    "\n",
    "repl_ens[:10],len(repl_ens), [r for r in repl_ens if len(r.split(' ')) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "334f0e6d-034a-4d2e-a446-2031135ae9fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Example: 100%|██████████| 66156/66156 [00:16<00:00, 3907.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents 66156.\n",
      "# of shuffle num 66156.\n"
     ]
    }
   ],
   "source": [
    "from itertools import groupby\n",
    "from copy import deepcopy, copy\n",
    "\n",
    "# 1. create [MASK] for all entity\n",
    "# 2. entity shuffle with next entity( all mention to the first mention)\n",
    "# 3. entity replaced with entity that out of distribution (NYT temporarily)\n",
    "\n",
    "#file_in = 'dataset/docred/dev.json'\n",
    "file_in = 'dataset/docred/dev_wo_overlap.json'\n",
    "\n",
    "tag = file_in.split('/')[-1].split('.')[0]\n",
    "\n",
    "i_line, pos_samples, neg_samples = 0, 0, 0\n",
    "ori_objs, en_mask_objs, en_shuf_objs, en_repl_objs = [], [], [], []\n",
    "mask_num, shuf_num, ood_num, ori_num = 0,0,0,0  \n",
    "MAX_SEQ_LENGTH = 1024\n",
    "MASK_TOKEN = tokenizer.mask_token\n",
    "\n",
    "limit = False\n",
    "with open(file_in, \"r\") as fh:\n",
    "    data = json.load(fh)\n",
    "if limit:\n",
    "    data = data[1109:1110]\n",
    "\n",
    "for sample in tqdm(data, desc=\"Example\"):\n",
    "    sents,labels = sample['sents'], sample['labels']\n",
    "    entities = sample['vertexSet']\n",
    "    title = sample['title']\n",
    "    all_mentions = []\n",
    "\n",
    "    # all entity mention and position\n",
    "    for ei,e in enumerate(entities):\n",
    "        for mi,m in enumerate(e):\n",
    "            all_mentions.append([m['pos'][0], m['pos'][1], m['sent_id'], m['type'], m['name'], ei, mi]) # e_ind and m_ind\n",
    "    all_mentions.sort(key=lambda x:(x[2],x[0]))\n",
    "\n",
    "    # 1) entity mask \n",
    "    en_mask_sents = deepcopy(sents)\n",
    "    for e in entities:\n",
    "        for m in e:\n",
    "            for po in range(m['pos'][0], m['pos'][1]):  # skip '*' token\n",
    "                tok_len = len(tokenizer.tokenize(sents[m['sent_id']][po]))\n",
    "                en_mask_sents[m['sent_id']][po] = tokenizer.mask_token * tok_len\n",
    "    en_mask_feature = {\n",
    "            'vertexSet': entities,\n",
    "            'labels': labels,\n",
    "            'title': title,\n",
    "            'sents': en_mask_sents\n",
    "        }\n",
    "    en_mask_objs.append(en_mask_feature)\n",
    "\n",
    "    # 2)entity shuffle (move to next entity position)\n",
    "    en_shuf_sents = deepcopy(sents)\n",
    "    shuf_sent_offset = defaultdict(int)\n",
    "    shuf_ens = []  # entity order after shuffle/shift\n",
    "    for e in entities:\n",
    "        shuf_ens.append(e[0]['name'])\n",
    "    shuf_ens = shuf_ens[-1:] + shuf_ens[:-1] # rotate to shuffle\n",
    "    shuf_mentions = []\n",
    "    for m in all_mentions:\n",
    "        sent_id = m[2]\n",
    "        offset = shuf_sent_offset[sent_id]\n",
    "        new_en = shuf_ens[m[-2]]\n",
    "        new_en = new_en.split()\n",
    "        new_st, new_ed = m[0]+offset, m[0]+len(new_en)+offset\n",
    "\n",
    "        shuf_mentions.append([new_st, new_ed, m[2], m[3], ' '.join(new_en)] + m[-2:])\n",
    "        ori_len = m[1] - m[0]\n",
    "        en_shuf_sents[sent_id] = en_shuf_sents[sent_id][:new_st] + new_en + en_shuf_sents[sent_id][new_st+ori_len:]\n",
    "        offset += len(new_en) - ori_len\n",
    "        shuf_sent_offset[sent_id] = offset\n",
    "\n",
    "    shuf_mentions.sort(key=lambda x:x[-2])\n",
    "    shuf_mens = [sorted([s for s in g], key=lambda y:y[-1]) for k, g in groupby(shuf_mentions, key = lambda x: x[-2])]  # need to groupby after sort\n",
    "    shuf_entities = []\n",
    "    for e in shuf_mens:\n",
    "        shuf_entities.append([])\n",
    "        for m in e:\n",
    "            shuf_entities[-1].append({\n",
    "                'name': m[4],\n",
    "                'pos': [m[0], m[1]],\n",
    "                'sent_id': m[2],\n",
    "                'type': m[3]\n",
    "            })\n",
    "    en_shuf_feature = {\n",
    "            'vertexSet': shuf_entities,\n",
    "            'labels': labels,\n",
    "            'title': title,\n",
    "            'sents': en_shuf_sents\n",
    "        }\n",
    "    en_shuf_objs.append(en_shuf_feature)\n",
    "\n",
    "    # 3)entity replacement (using ood entity name)\n",
    "    en_repl_sents, repl_mentions = deepcopy(sents), []\n",
    "    repl_sent_offset = defaultdict(int)\n",
    "    repl_mentions = []\n",
    "    for m in all_mentions:\n",
    "        sent_id = m[2]\n",
    "        offset = repl_sent_offset[sent_id]\n",
    "        new_en = repl_ens[m[-2] * 10] # fetch every 10 entities\n",
    "        new_en = new_en.split()\n",
    "        new_st,new_ed = m[0]+offset, m[0]+len(new_en)+offset\n",
    "        \n",
    "        repl_mentions.append([new_st, new_ed, m[2], m[3], ' '.join(new_en)] + m[-2:])\n",
    "        ori_len = m[1] - m[0]\n",
    "        en_repl_sents[sent_id] = en_repl_sents[sent_id][:new_st] + new_en + en_repl_sents[sent_id][new_st+ori_len:]\n",
    "        offset += len(new_en) - ori_len\n",
    "        repl_sent_offset[sent_id] = offset\n",
    "\n",
    "    repl_mentions.sort(key=lambda x: x[-2])\n",
    "    repl_mens = [sorted([s for s in g], key=lambda y:y[-1]) for k, g in groupby(repl_mentions, key = lambda x: x[-2])]\n",
    "    repl_entities = []\n",
    "    for e in repl_mens:\n",
    "        repl_entities.append([])\n",
    "        for m in e:\n",
    "            repl_entities[-1].append({\n",
    "                'name': m[4],\n",
    "                'pos': [m[0], m[1]],\n",
    "                'sent_id': m[2],\n",
    "                'type': m[3]\n",
    "            })\n",
    "    en_repl_feature = {\n",
    "            'vertexSet': repl_entities,\n",
    "            'labels': labels,\n",
    "            'title': title,\n",
    "            'sents': en_repl_sents\n",
    "        }\n",
    "    en_repl_objs.append(en_repl_feature)\n",
    "\n",
    "    i_line += 1\n",
    "\n",
    "attack_dir = 'attack_pkl/'\n",
    "# json.dump(ori_features, open(attack_dir + model_type + '@ori_keyword_dev.pkl', 'wb'))\n",
    "en_mask_path = attack_dir + tag + '@' + model_type + '@en_mask_dev.json'\n",
    "en_shuf_path = attack_dir + tag + '@' + model_type + '@en_shuf_dev.json'\n",
    "en_repl_path = attack_dir + tag + '@' + model_type + '@en_repl_dev.json'\n",
    "ori_objs = json.load(open(file_in))\n",
    "\n",
    "json.dump(en_mask_objs, open(en_mask_path, 'w'))\n",
    "json.dump(en_shuf_objs, open(en_shuf_path, 'w'))\n",
    "json.dump(en_repl_objs, open(en_repl_path, 'w'))\n",
    "print(\"# of documents {}.\".format(i_line))\n",
    "# print(\"# of original num {}.\".format(ori_num))\n",
    "print(\"# of shuffle num {}.\".format(len(en_shuf_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aca27439-0232-46a5-9dfd-d992e546b107",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'datsaset': 'scirec_test', 'title': 'scirec_test_0', 'sents': [['Recognition', 'of', 'proper', 'nouns', 'in', 'Japanese', 'text', 'has', 'been', 'studied', 'as', 'a', 'part', 'of', 'the', 'more', 'general', 'problem', 'of', 'morphological', 'analysis', 'in', 'Japanese', 'text', 'processing', '(', '[', '1', ']', '[', '2', ']', ')', '.']], 'vertexSet': [[{'sent_id': 0, 'pos': [0, 4], 'name': 'Recognition of proper nouns', 'type': 'TASK'}], [{'sent_id': 0, 'pos': [2, 4], 'name': 'proper nouns', 'type': 'OTHERSCIENTIFICTERM'}], [{'sent_id': 0, 'pos': [5, 7], 'name': 'Japanese text', 'type': 'MATERIAL'}], [{'sent_id': 0, 'pos': [19, 21], 'name': 'morphological analysis', 'type': 'TASK'}], [{'sent_id': 0, 'pos': [22, 25], 'name': 'Japanese text processing', 'type': 'TASK'}]], 'labels': [{'r': 'P361', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P361', 'h': 1, 't': 2, 'evidence': [0]}]}\n",
      "\n",
      "{'vertexSet': [[{'name': 'Japanese text processing', 'pos': [0, 3], 'sent_id': 0, 'type': 'TASK'}], [{'name': 'Recognition of proper nouns', 'pos': [1, 5], 'sent_id': 0, 'type': 'OTHERSCIENTIFICTERM'}], [{'name': 'proper nouns', 'pos': [6, 8], 'sent_id': 0, 'type': 'MATERIAL'}], [{'name': 'Japanese text', 'pos': [20, 22], 'sent_id': 0, 'type': 'TASK'}], [{'name': 'morphological analysis', 'pos': [23, 25], 'sent_id': 0, 'type': 'TASK'}]], 'labels': [{'r': 'P361', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P361', 'h': 1, 't': 2, 'evidence': [0]}], 'title': 'scirec_test_0', 'sents': [['Japanese', 'Recognition', 'of', 'proper', 'nouns', 'in', 'proper', 'nouns', 'has', 'been', 'studied', 'as', 'a', 'part', 'of', 'the', 'more', 'general', 'problem', 'of', 'Japanese', 'text', 'in', 'morphological', 'analysis', '(', '[', '1', ']', '[', '2', ']', ')', '.']]}\n",
      "\n",
      "{'vertexSet': [[{'sent_id': 0, 'pos': [0, 4], 'name': 'Recognition of proper nouns', 'type': 'TASK'}], [{'sent_id': 0, 'pos': [2, 4], 'name': 'proper nouns', 'type': 'OTHERSCIENTIFICTERM'}], [{'sent_id': 0, 'pos': [5, 7], 'name': 'Japanese text', 'type': 'MATERIAL'}], [{'sent_id': 0, 'pos': [19, 21], 'name': 'morphological analysis', 'type': 'TASK'}], [{'sent_id': 0, 'pos': [22, 25], 'name': 'Japanese text processing', 'type': 'TASK'}]], 'labels': [{'r': 'P361', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P361', 'h': 1, 't': 2, 'evidence': [0]}], 'title': 'scirec_test_0', 'sents': [['<mask><mask><mask>', '<mask>', '<mask><mask>', '<mask><mask><mask>', 'in', '<mask>', '<mask>', 'has', 'been', 'studied', 'as', 'a', 'part', 'of', 'the', 'more', 'general', 'problem', 'of', '<mask><mask>', '<mask>', 'in', '<mask>', '<mask>', '<mask>', '(', '[', '1', ']', '[', '2', ']', ')', '.']]}\n",
      "\n",
      "{'vertexSet': [[{'name': 'Ravelli', 'pos': [0, 1], 'sent_id': 0, 'type': 'TASK'}], [{'name': 'Marin', 'pos': [-1, 0], 'sent_id': 0, 'type': 'OTHERSCIENTIFICTERM'}], [{'name': 'Alkmaar', 'pos': [1, 2], 'sent_id': 0, 'type': 'MATERIAL'}], [{'name': 'Nesher', 'pos': [14, 15], 'sent_id': 0, 'type': 'TASK'}], [{'name': 'Pischetsrieder', 'pos': [16, 17], 'sent_id': 0, 'type': 'TASK'}]], 'labels': [{'r': 'P361', 'h': 0, 't': 3, 'evidence': [0]}, {'r': 'P361', 'h': 1, 't': 2, 'evidence': [0]}], 'title': 'scirec_test_0', 'sents': [['Ravelli', 'Alkmaar', 'text', 'has', 'been', 'studied', 'as', 'a', 'part', 'of', 'the', 'more', 'general', 'problem', 'Nesher', 'analysis', 'Pischetsrieder', 'processing', '(', '[', '1', ']', '[', '2', ']', ')', 'Marin', 'in', 'Japanese', 'text', 'has', 'been', 'studied', 'as', 'a', 'part', 'of', 'the', 'more', 'general', 'problem', 'of', 'morphological', 'analysis', 'in', 'Japanese', 'text', 'processing', '(', '[', '1', ']', '[', '2', ']', ')', '.']]}\n"
     ]
    }
   ],
   "source": [
    "idx = 51748\n",
    "print(ori_objs[idx])\n",
    "print()\n",
    "print(en_shuf_objs[idx])\n",
    "print()\n",
    "print(en_mask_objs[idx])\n",
    "print()\n",
    "print(en_repl_objs[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71b5bb-be4e-4da5-a1d1-ebd5fa8e29d4",
   "metadata": {},
   "source": [
    "## Load entity attack dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339ef2ef-7872-452e-979a-ebbd415031d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = 'dataset/docred/dev_wo_overlap.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fcc600c3-e222-4f44-821b-4fba5abb7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_dir = 'attack_pkl/'\n",
    "# json.dump(ori_features, open(attack_dir + model_type + '@ori_keyword_dev.pkl', 'wb'))\n",
    "tag = file_in.split('/')[-1].split('.')[0]\n",
    "\n",
    "en_mask_path = attack_dir + tag + '@' + model_type + '@en_mask_dev.json'\n",
    "en_shuf_path = attack_dir + tag + '@' + model_type + '@en_shuf_dev.json'\n",
    "en_repl_path = attack_dir + tag + '@' + model_type + '@en_repl_dev.json'\n",
    "\n",
    "# ori_json = json.load(open(file_in))\n",
    "# en_mask_json = json.load(open(attack_dir + model_type + '@en_mask_dev.json'))\n",
    "# en_shuf_json = json.load(open(attack_dir + model_type + '@en_shuf_dev.json'))\n",
    "# en_repl_json = json.load(open(attack_dir + model_type + '@en_repl_dev.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba7156d-f24c-4a65-8dda-27215a1f5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_features = read_docred(file_in, tokenizer)\n",
    "en_mask_features = read_docred(en_mask_path, tokenizer)\n",
    "en_shuf_features = read_docred(en_shuf_path, tokenizer)\n",
    "en_repl_features = read_docred(en_repl_path, tokenizer)\n",
    "len(en_mask_features), len(en_shuf_features), len(en_repl_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f34994-2547-442d-9f14-f0c11af7b940",
   "metadata": {},
   "source": [
    "## Run entity attack on dev_wo_overlap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "393202e8-356a-4f77-a67e-4d3d88c6d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 60.76858731486493, 'dev_F1_ign': 58.95526998802616}\n",
      "{'dev_F1': 6.39078051335778, 'dev_F1_ign': 6.390185517497591}\n",
      "{'dev_F1': 6.084205402774398, 'dev_F1_ign': 5.689488987412854}\n",
      "{'dev_F1': 14.16457286432161, 'dev_F1_ign': 14.09384602035105}\n",
      "& original & 60.76858731486493 & 58.95526998802616 \\\\ \n",
      "& entity mask & 6.39078051335778 & 6.390185517497591 \\\\ \n",
      "& entity move & 6.084205402774398 & 5.689488987412854 \\\\ \n",
      "& entity replace & 14.16457286432161 & 14.09384602035105 \\\\ \n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "## bert on dev_wo_overlap\n",
    "all_feas = [ori_features, en_mask_features, en_shuf_features, en_repl_features] \n",
    "f1_outs = []\n",
    "args.dev_file = 'dev_wo_overlap.json'\n",
    "for fea in all_feas:\n",
    "    _, f1_out = evaluate(args, model, fea, tag=\"dev\")\n",
    "    print(f1_out)\n",
    "    f1_outs.append(f1_out)\n",
    "en_attack_strs = ['original', 'entity mask', 'entity move', 'entity replace']\n",
    "for i in range(0, len(f1_outs)):\n",
    "    print(f'& {en_attack_strs[i]} & {f1_outs[i][\"dev_F1\"]} & {f1_outs[i][\"dev_F1_ign\"]} \\\\\\\\ ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c685c8c6-a200-46ef-9736-9ec602169b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_F1': 61.62089105565478, 'dev_F1_ign': 59.744852760161}\n",
      "{'dev_F1': 27.28648648648648, 'dev_F1_ign': 27.083648897895486}\n",
      "{'dev_F1': 7.352144876962879, 'dev_F1_ign': 6.96639315184737}\n",
      "{'dev_F1': 17.497299799413675, 'dev_F1_ign': 17.406135449708817}\n",
      "& original & 61.62089105565478 & 59.744852760161 \\\\ \n",
      "& entity mask & 27.28648648648648 & 27.083648897895486 \\\\ \n",
      "& entity move & 7.352144876962879 & 6.96639315184737 \\\\ \n",
      "& entity replace & 17.497299799413675 & 17.406135449708817 \\\\ \n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "## roberta-large original on dev_wo_overlap\n",
    "all_feas = [ori_features, en_mask_features, en_shuf_features, en_repl_features] \n",
    "f1_outs = []\n",
    "args.dev_file = 'dev_wo_overlap.json'\n",
    "for fea in all_feas:\n",
    "    _, f1_out = evaluate(args, model, fea, tag=\"dev\")\n",
    "    print(f1_out)\n",
    "    f1_outs.append(f1_out)\n",
    "en_attack_strs = ['original', 'entity mask', 'entity move', 'entity replace']\n",
    "for i in range(0, len(f1_outs)):\n",
    "    print(f'& {en_attack_strs[i]} & {f1_outs[i][\"dev_F1\"]} & {f1_outs[i][\"dev_F1_ign\"]} \\\\\\\\ ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlop",
   "language": "python",
   "name": "atlop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
